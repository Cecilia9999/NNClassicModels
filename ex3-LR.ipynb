{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ex3-LR.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP3BuQVyE0k4DGWJhtTnE5a"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"ytiknXNFplay","colab_type":"code","colab":{}},"source":["import torch \n","import torch.nn as nn\n","import torchvision\n","import torchvision.transforms as transforms\n","%matplotlib inline\n","\n","# hyper-paramters\n","input_size = 28 * 28    # 784\n","num_classes = 10\n","num_epochs = 5\n","batch_size = 100\n","learning_rate = 0.001\n","\n","# MNIST dataset (images and labels)\n","train_dataset = torchvision.datasets.MNIST(root='./data/',\n","                                           train=True,      # training data\n","                                           transform=transforms.ToTensor(),\n","                                           download=True\n","                                           )\n","\n","test_dataset = torchvision.datasets.MNIST(root='./data/',\n","                                          train=False,      # not training data                                           \n","                                          transform=transforms.ToTensor()  \n","                                          )\n","\n","# Data loader (imput pipeline)\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n","                                           batch_size=batch_size,\n","                                           shuffle=True\n","                                           )\n","\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n","                                          batch_size=batch_size,\n","                                          shuffle=False\n","                                          )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XlyQyNGdXT6T","colab_type":"text"},"source":["在train_loader里，总数据分为batch_size=100的N批数据，每个数据的尺寸是（1,28,28）灰度x尺寸\n","因此，N批中的第i批，i[0]是x，维度为 (100,1,28,28)；\n","i[1]是y，维度为 (100,1) \n"]},{"cell_type":"code","metadata":{"id":"-4_3awI8ZrAD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":333},"executionInfo":{"status":"ok","timestamp":1593692326411,"user_tz":-540,"elapsed":849,"user":{"displayName":"Cecilia SS","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhZdujNRISiRFR_Nh34nbB8-fEtGSvGXr2rEpf5=s64","userId":"04154856476552279883"}},"outputId":"c44caef2-88d8-4ed1-94c2-5f0d2107b503"},"source":["dataiter = iter(train_loader)\n","images, labels = dataiter.next()\n","print(type(images))\n","print(images.shape)\n","print(labels.shape)\n","\n","# see how the images are\n","plt.imshow(images[99].numpy().squeeze())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<class 'torch.Tensor'>\n","torch.Size([100, 1, 28, 28])\n","torch.Size([100])\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7fb71c57c198>"]},"metadata":{"tags":[]},"execution_count":67},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"T0t5yqBMJBKA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":527},"executionInfo":{"status":"ok","timestamp":1593693982230,"user_tz":-540,"elapsed":21765,"user":{"displayName":"Cecilia SS","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhZdujNRISiRFR_Nh34nbB8-fEtGSvGXr2rEpf5=s64","userId":"04154856476552279883"}},"outputId":"3ecebd13-ed50-4d8d-809d-9d93e71dbe71"},"source":["# logistice regression\n","# p.s. same as Linear regression model but different loss func\n","model = nn.Linear(input_size, num_classes) # input 28*28, output 0~9: 10\n","\n","# loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n","\n","# train model\n","# in one epoch, total iteration = batch number (= total data/batch size)\n","total_step = len(train_loader)  \n","for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):\n","        # reshape images to (batch_size, input_size) cuz input x should be 1D vec\n","        images = images.reshape(-1, input_size) # (any, 1*28*28)\n","\n","        # forward\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","\n","        # backwards and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if (i+1) % 100 == 0:\n","            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n","                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch [1/5], Step [100/600], Loss: 2.2227\n","Epoch [1/5], Step [200/600], Loss: 2.1744\n","Epoch [1/5], Step [300/600], Loss: 2.0181\n","Epoch [1/5], Step [400/600], Loss: 1.9891\n","Epoch [1/5], Step [500/600], Loss: 1.9127\n","Epoch [1/5], Step [600/600], Loss: 1.7951\n","Epoch [2/5], Step [100/600], Loss: 1.7867\n","Epoch [2/5], Step [200/600], Loss: 1.7028\n","Epoch [2/5], Step [300/600], Loss: 1.5434\n","Epoch [2/5], Step [400/600], Loss: 1.5518\n","Epoch [2/5], Step [500/600], Loss: 1.5591\n","Epoch [2/5], Step [600/600], Loss: 1.5238\n","Epoch [3/5], Step [100/600], Loss: 1.4215\n","Epoch [3/5], Step [200/600], Loss: 1.3254\n","Epoch [3/5], Step [300/600], Loss: 1.4089\n","Epoch [3/5], Step [400/600], Loss: 1.2711\n","Epoch [3/5], Step [500/600], Loss: 1.3304\n","Epoch [3/5], Step [600/600], Loss: 1.2024\n","Epoch [4/5], Step [100/600], Loss: 1.2159\n","Epoch [4/5], Step [200/600], Loss: 1.1285\n","Epoch [4/5], Step [300/600], Loss: 1.2372\n","Epoch [4/5], Step [400/600], Loss: 1.1474\n","Epoch [4/5], Step [500/600], Loss: 1.1692\n","Epoch [4/5], Step [600/600], Loss: 1.0855\n","Epoch [5/5], Step [100/600], Loss: 1.0842\n","Epoch [5/5], Step [200/600], Loss: 1.1266\n","Epoch [5/5], Step [300/600], Loss: 0.9852\n","Epoch [5/5], Step [400/600], Loss: 1.0732\n","Epoch [5/5], Step [500/600], Loss: 1.0437\n","Epoch [5/5], Step [600/600], Loss: 0.8538\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IqdQBYnRf_HZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1593694538478,"user_tz":-540,"elapsed":1214,"user":{"displayName":"Cecilia SS","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhZdujNRISiRFR_Nh34nbB8-fEtGSvGXr2rEpf5=s64","userId":"04154856476552279883"}},"outputId":"014c0782-b0b8-4516-bb98-97a7097b4ae1"},"source":["# Test model\n","with torch.no_grad():\n","    correct = 0\n","    total = 0\n","    for images, labels in test_loader:\n","        images = images.reshape(-1, input_size)\n","        outputs = model(images)\n","        # print(outputs.data.size())  [100, 10:num_classes]\n","        _, predict = torch.max(outputs.data, 1)\n","        \n","        # one iteration has totally batch size data\n","        \n","        total += labels.size(0)\n","        correct += (predict == labels).sum()\n","\n","    # coorect is a tensor --> use .item() get int value       \n","    print('Accuracy of model is {}'.format(correct.item()/total*100))\n","\n","# save model checkpoint\n","# torch.save(model.state_dict(), 'model.pkl')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["10000 8272\n","Accuracy of model is 82.72\n"],"name":"stdout"}]}]}