{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CNN.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMwRCDZ0nQ9b+NoamhWa3Hn"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"aDF4KpR6TVGn","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593747273394,"user_tz":-540,"elapsed":1167,"user":{"displayName":"Cecilia SS","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhZdujNRISiRFR_Nh34nbB8-fEtGSvGXr2rEpf5=s64","userId":"04154856476552279883"}}},"source":["import torch\n","import torch.nn as nn\n","import torchvision\n","import torchvision.transforms as transforms\n","%matplotlib inline"],"execution_count":42,"outputs":[]},{"cell_type":"code","metadata":{"id":"vzGYXwVdWTWW","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593747275541,"user_tz":-540,"elapsed":965,"user":{"displayName":"Cecilia SS","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhZdujNRISiRFR_Nh34nbB8-fEtGSvGXr2rEpf5=s64","userId":"04154856476552279883"}}},"source":["# device config\n","# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Hyper parameters\n","num_epochs = 5\n","num_classes = 10\n","batch_size = 100\n","learning_rate = 0.001"],"execution_count":43,"outputs":[]},{"cell_type":"code","metadata":{"id":"xwlsYZfDWyQI","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593747286824,"user_tz":-540,"elapsed":999,"user":{"displayName":"Cecilia SS","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhZdujNRISiRFR_Nh34nbB8-fEtGSvGXr2rEpf5=s64","userId":"04154856476552279883"}}},"source":["# MNIST dataset \n","train_dataset = torchvision.datasets.MNIST(root='./data/',\n","                                           train=True,\n","                                           download=True,\n","                                           transform=transforms.ToTensor()\n","                                           )\n","\n","test_dataset = torchvision.datasets.MNIST(root='./data/',\n","                                           train=False,\n","                                           transform=transforms.ToTensor()\n","                                           )\n","\n","# data loader\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n","                                           shuffle=True,\n","                                           batch_size=batch_size\n","                                           #num_workers=2\n","                                           )\n","\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n","                                           shuffle=False,\n","                                           batch_size=batch_size\n","                                           #num_workers=2\n","                                           )"],"execution_count":44,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gt5XU7UzYCV8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":527},"executionInfo":{"status":"ok","timestamp":1593747518754,"user_tz":-540,"elapsed":229336,"user":{"displayName":"Cecilia SS","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhZdujNRISiRFR_Nh34nbB8-fEtGSvGXr2rEpf5=s64","userId":"04154856476552279883"}},"outputId":"ad6ddcec-0400-483b-b893-eb82cb542eb2"},"source":["# convolutional NN (2 layers)\n","class ConvNN(nn.Module):\n","    def __init__(self, num_classes=10):\n","        super(ConvNN, self).__init__()\n","        self.layer1 = nn.Sequential(\n","                      # Conv2d(Channels, Kernel_num)\n","                      nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n","                      nn.BatchNorm2d(16),\n","                      nn.ReLU(),\n","                      nn.MaxPool2d(kernel_size=2, stride=2)\n","                                    ) \n","        self.layer2 = nn.Sequential(\n","                      nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n","                      nn.BatchNorm2d(32),\n","                      nn.ReLU(),\n","                      nn.MaxPool2d(kernel_size=2, stride=2)\n","                                    )\n","        # layer1,2 output size=(28 - 5 + 2p) + 1 = 28\n","\n","        self.fc = nn.Linear(32*7*7, num_classes)        # Linear(in, out)   \n","    \n","    def forward(self, x):\n","        out1 = self.layer1(x)\n","        out2 = self.layer2(out1)\n","        out2 = out2.reshape(out2.size(0), -1)  # flatten the data(batchsize, 1d)\n","        out = self.fc(out2)\n","        return out\n","\n","model = ConvNN(num_classes) \n","# model = ConvNN(num_classes).to(device)    # use gpu\n","\n","model.train()       # BN and dropout used train model\n","\n","# define loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# train model\n","total_step = len(train_loader)\n","for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):\n","        # Note, send x into cnn, don't care the dimension of input\n","        # but send x into fc, should change input x into (-1, fc_inputsize)\n","        # images = images.to(device)    # use gpu\n","        # labels = labels.to(device)    # use gpu\n","\n","        # forward\n","        output = model(images)\n","        loss = criterion(output, labels)\n","\n","        # backward\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if (i+1) % 100 == 0:\n","            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n","                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"],"execution_count":45,"outputs":[{"output_type":"stream","text":["Epoch [1/5], Step [100/600], Loss: 0.1875\n","Epoch [1/5], Step [200/600], Loss: 0.0777\n","Epoch [1/5], Step [300/600], Loss: 0.1983\n","Epoch [1/5], Step [400/600], Loss: 0.1088\n","Epoch [1/5], Step [500/600], Loss: 0.0877\n","Epoch [1/5], Step [600/600], Loss: 0.0621\n","Epoch [2/5], Step [100/600], Loss: 0.0509\n","Epoch [2/5], Step [200/600], Loss: 0.0090\n","Epoch [2/5], Step [300/600], Loss: 0.0413\n","Epoch [2/5], Step [400/600], Loss: 0.0361\n","Epoch [2/5], Step [500/600], Loss: 0.1294\n","Epoch [2/5], Step [600/600], Loss: 0.0178\n","Epoch [3/5], Step [100/600], Loss: 0.0168\n","Epoch [3/5], Step [200/600], Loss: 0.0273\n","Epoch [3/5], Step [300/600], Loss: 0.0072\n","Epoch [3/5], Step [400/600], Loss: 0.0048\n","Epoch [3/5], Step [500/600], Loss: 0.0041\n","Epoch [3/5], Step [600/600], Loss: 0.0142\n","Epoch [4/5], Step [100/600], Loss: 0.0397\n","Epoch [4/5], Step [200/600], Loss: 0.0056\n","Epoch [4/5], Step [300/600], Loss: 0.0443\n","Epoch [4/5], Step [400/600], Loss: 0.0290\n","Epoch [4/5], Step [500/600], Loss: 0.0113\n","Epoch [4/5], Step [600/600], Loss: 0.0449\n","Epoch [5/5], Step [100/600], Loss: 0.0059\n","Epoch [5/5], Step [200/600], Loss: 0.0264\n","Epoch [5/5], Step [300/600], Loss: 0.0016\n","Epoch [5/5], Step [400/600], Loss: 0.0510\n","Epoch [5/5], Step [500/600], Loss: 0.0190\n","Epoch [5/5], Step [600/600], Loss: 0.0183\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XK1GJse3mwk-","colab_type":"text"},"source":["BN和Dropout一般有两种模式，一个是训练时使用（训练前加上model.train()），\n","另一个测试时使用，**测试前要加上model.eval()**，使batch Norm使用训练好的值而非求平均值。\n","\n","\n","*   ***model.eval()*** will notify all your layers that you are in eval mode, that way, batchnorm or dropout layers will work in eval mode instead of training mode.\n","\n","为什么测试不使用梯度\n","*   ***torch.no_grad()*** impacts the autograd engine and deactivate it. It will reduce memory usage and speed up computations but you won’t be able to backprop (which you don’t want in an eval script)."]},{"cell_type":"code","metadata":{"id":"urqVVjD6mTVg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593747057434,"user_tz":-540,"elapsed":9659,"user":{"displayName":"Cecilia SS","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhZdujNRISiRFR_Nh34nbB8-fEtGSvGXr2rEpf5=s64","userId":"04154856476552279883"}},"outputId":"048fd422-9f34-428e-9894-afde3187256c"},"source":["# test model\n","model.eval() \n","with torch.no_grad():\n","    correct = 0\n","    total = 0\n","    for images, labels in test_loader:\n","        #images = images.to(device)\n","        #labels = labels.to(device)\n","        output = model(images)\n","        prob, predict = torch.max(output, 1) \n","        correct += (predict == labels).sum().item()\n","        total += labels.size(0)\n","\n","    print('Accuracy of the model is: {}'.format(100 * correct / total))\n","\n","# save model checkpoint\n","# torch.save(model.state_dict(), 'model.pkl')"],"execution_count":41,"outputs":[{"output_type":"stream","text":["Accuracy of the model is: 98.98\n"],"name":"stdout"}]}]}